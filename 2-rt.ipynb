{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import RewardTrainer\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_training = False\n",
    "saveModel = False\n",
    "checkInitialAccuracy = False\n",
    "score_ft_model_inferences = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveModel:\n",
    "  from huggingface_hub import login\n",
    "  access_token = \"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\"\n",
    "  login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"cahya/distilbert-base-indonesian\"\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "model_name = \"w11wo/indonesian-roberta-base-sentiment-classifier\"\n",
    "if reward_training:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "else:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"./reward_model\", num_labels=3, load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RLHF dataset\n",
    "if reward_training:\n",
    "    dataset_path = \"./dataset/4-way_comparison_labelled.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=127)\n",
    "\n",
    "    def prepare_rlhf_data(df):\n",
    "        rows = []\n",
    "        for idx, row in df.iterrows():\n",
    "            prompt = row['x']\n",
    "            chosen_idx = row['b']\n",
    "            chosen_response = row[f'y{chosen_idx}']\n",
    "            \n",
    "            for i in range(4):\n",
    "                if i != chosen_idx:\n",
    "                    rejected_response = row[f'y{i}']\n",
    "                    if chosen_response != rejected_response:\n",
    "                        rows.append({\n",
    "                            \"instruction\": prompt,\n",
    "                            \"chosen_response\": chosen_response,\n",
    "                            \"rejected_response\": rejected_response\n",
    "                        })\n",
    "        \n",
    "        return Dataset.from_list(rows)\n",
    "\n",
    "    # Prepare RLHF data for training and test sets\n",
    "    train_rlhf_dataset = prepare_rlhf_data(train_df)\n",
    "    test_rlhf_dataset = prepare_rlhf_data(test_df)\n",
    "\n",
    "    # Define the formatting function\n",
    "    def formatting_func(examples):\n",
    "        input_ids_chosen = []\n",
    "        attention_mask_chosen = []\n",
    "        input_ids_rejected = []\n",
    "        attention_mask_rejected = []\n",
    "        \n",
    "        kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "\n",
    "        for instruction, chosen_response, rejected_response in zip(examples[\"instruction\"], examples[\"chosen_response\"], examples[\"rejected_response\"]):\n",
    "            prompt_plus_chosen_response = instruction + \"\\n\" + chosen_response\n",
    "            prompt_plus_rejected_response = instruction + \"\\n\" + rejected_response\n",
    "            tokens_chosen = tokenizer(prompt_plus_chosen_response, **kwargs)\n",
    "            tokens_rejected = tokenizer(prompt_plus_rejected_response, **kwargs)\n",
    "            \n",
    "            input_ids_chosen.append(tokens_chosen[\"input_ids\"][0])\n",
    "            attention_mask_chosen.append(tokens_chosen[\"attention_mask\"][0])\n",
    "            input_ids_rejected.append(tokens_rejected[\"input_ids\"][0])\n",
    "            attention_mask_rejected.append(tokens_rejected[\"attention_mask\"][0])\n",
    "\n",
    "        return {\n",
    "            \"input_ids_chosen\": input_ids_chosen,\n",
    "            \"attention_mask_chosen\": attention_mask_chosen,\n",
    "            \"input_ids_rejected\": input_ids_rejected,\n",
    "            \"attention_mask_rejected\": attention_mask_rejected\n",
    "        }\n",
    "\n",
    "    # Format the datasets\n",
    "    formatted_train_dataset = train_rlhf_dataset.map(formatting_func, batched=True)\n",
    "    formatted_test_dataset = test_rlhf_dataset.map(formatting_func, batched=True)\n",
    "\n",
    "    # Combine the formatted datasets into a dictionary\n",
    "    formatted_dataset = {\n",
    "        \"train\": formatted_train_dataset,\n",
    "        \"test\": formatted_test_dataset\n",
    "    }\n",
    "\n",
    "    # Print the first example from the formatted training dataset\n",
    "    for i in range(1):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(\"Input IDs (Chosen):\", formatted_dataset[\"train\"][\"input_ids_chosen\"][i])\n",
    "        print(\"Attention Mask (Chosen):\", formatted_dataset[\"train\"][\"attention_mask_chosen\"][i])\n",
    "        print(\"Input IDs (Rejected):\", formatted_dataset[\"train\"][\"input_ids_rejected\"][i])\n",
    "        print(\"Attention Mask (Rejected):\", formatted_dataset[\"train\"][\"attention_mask_rejected\"][i])\n",
    "        print()\n",
    "\n",
    "    print(\"Number of training examples:\", len(formatted_dataset[\"train\"]))\n",
    "    print(\"Number of testing examples:\", len(formatted_dataset[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reward_training and checkInitialAccuracy:\n",
    "    # Define predict_response function\n",
    "    def predict_response(model, tokenizer, input_ids, attention_mask):\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "    # Assuming formatted_dataset is already defined and split into train and test\n",
    "    test_dataset = formatted_dataset['test']\n",
    "\n",
    "    # Initialize variables to store predicted responses and ground truth labels\n",
    "    predicted_responses = []\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    # Loop through each example in the test dataset\n",
    "    for i in range(len(test_dataset)):\n",
    "        input_ids_chosen = torch.tensor(test_dataset['input_ids_chosen'][i])  # Convert list to tensor\n",
    "        attention_mask_chosen = torch.tensor(test_dataset['attention_mask_chosen'][i])  # Convert list to tensor\n",
    "        input_ids_rejected = torch.tensor(test_dataset['input_ids_rejected'][i])  # Convert list to tensor\n",
    "        attention_mask_rejected = torch.tensor(test_dataset['attention_mask_rejected'][i])  # Convert list to tensor\n",
    "\n",
    "        # Predict chosen response logits\n",
    "        logits_chosen = predict_response(model, tokenizer, input_ids_chosen.unsqueeze(0), attention_mask_chosen.unsqueeze(0)).squeeze()\n",
    "        # Predict rejected response logits\n",
    "        logits_rejected = predict_response(model, tokenizer, input_ids_rejected.unsqueeze(0), attention_mask_rejected.unsqueeze(0)).squeeze()\n",
    "\n",
    "        # Determine preferred response based on logits\n",
    "        chosen_sum = logits_chosen.sum().item()\n",
    "        rejected_sum = logits_rejected.sum().item()\n",
    "\n",
    "        if chosen_sum > rejected_sum:\n",
    "            predicted_response = 'chosen_response'\n",
    "        else:\n",
    "            predicted_response = 'rejected_response'\n",
    "\n",
    "        # Append predicted response and ground truth label\n",
    "        predicted_responses.append(predicted_response)\n",
    "        ground_truth_label = 'chosen_response' if i % 2 == 0 else 'rejected_response'  # Example ground truth labeling\n",
    "        ground_truth_labels.append(ground_truth_label)  # Replace with actual ground truth labels from your dataset\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct_predictions = sum(1 for pred, true in zip(predicted_responses, ground_truth_labels) if pred == true)\n",
    "    total_examples = len(ground_truth_labels)\n",
    "    accuracy = correct_predictions / total_examples * 100\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f\"\\nAccuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the training arguments\n",
    "if reward_training:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./train-reward-training\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs = 8,\n",
    "        learning_rate = 1e-5,\n",
    "        optim='adamw_torch',\n",
    "        overwrite_output_dir='True',\n",
    "        push_to_hub=saveModel,\n",
    "        fp16=True,\n",
    "        report_to=None,\n",
    "    )\n",
    "    # Loading the RewardTrainer from TRL\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=formatted_dataset[\"train\"],\n",
    "        eval_dataset=formatted_dataset[\"test\"],\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reward_training:\n",
    "  trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveModel:\n",
    "  model.save_pretrained(\"./reward_model\")\n",
    "  trainer.push_to_hub(\"Adzka/reward-model-distilbert-indo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predict_response function\n",
    "def predict_response(model, tokenizer, input_ids, attention_mask):\n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    return logits\n",
    "\n",
    "# -----\n",
    "\n",
    "if score_ft_model_inferences:\n",
    "    # Load the RLHF dataset\n",
    "    dataset_path = \"./inferences/ft_model_rl_data_for_PPO.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Initialize variables to store predicted scores\n",
    "    scores = []\n",
    "\n",
    "    n_test = len(df)\n",
    "    # Loop through each example in the test dataset\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row['query']\n",
    "        response = row['response']\n",
    "\n",
    "        kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "        prompt_plus_response = query + \"\\n\" + response\n",
    "\n",
    "        tokens = tokenizer(prompt_plus_response, **kwargs)\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        # Predict logits for the response\n",
    "        logits = predict_response(model, tokenizer, input_ids, attention_mask)\n",
    "        score = logits.sum().item()\n",
    "\n",
    "        # Append predicted score\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"Processed example {idx+1} out of {n_test}\")\n",
    "\n",
    "    # Add 'score' column to dataframe\n",
    "    df['score'] = scores\n",
    "\n",
    "    # Save the updated dataframe to CSV\n",
    "    output_csv_path = \"./inferences/ft_model_rl_data_for_PPO_scored.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
