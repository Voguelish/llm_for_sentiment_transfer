{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from rouge import Rouge\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune = False\n",
    "saveModel = False\n",
    "zero_shot_inference = False\n",
    "ft_inference = True\n",
    "print_evaluation = False\n",
    "generate_4_way = False\n",
    "rl_data_inference = False\n",
    "examples_to_compare = 5\n",
    "\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_dataset = load_dataset(\"json\", data_files=\"./dataset/ft_train.json\")[\"train\"]\n",
    "val_dataset = load_dataset(\"json\", data_files=\"./dataset/ft_test.json\")[\"train\"]\n",
    "rl_dataset = load_dataset(\"json\", data_files=\"./dataset/rl_data.json\")[\"train\"]\n",
    "test_dataset = load_dataset(\"json\", data_files=\"./dataset/rl_eval.json\")[\"train\"]\n",
    "\n",
    "examples_to_compare = min(len(test_dataset), examples_to_compare)\n",
    "\n",
    "# Print the sizes of each dataset\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"RL dataset size:\", len(rl_dataset))\n",
    "print(\"Test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"aisingapore/sea-lion-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saveModel:\n",
    "  from huggingface_hub import login\n",
    "  access_token = \"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\"\n",
    "  login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune or zero_shot_inference:\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True, load_in_4bit=load_in_4bit)\n",
    "  prompt_template = \"### USER:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
    "\n",
    "  prompt = \"\"\"Ubah sentimen dari kalimat awal berikut menjadi sentimen sebaliknya\n",
    "  Kalimat Awal: Hari ini langitnya sangat cerah dan ramah untuk jalan-jalan\n",
    "  Kalimat Baru:\"\"\"\n",
    "\n",
    "  full_prompt = prompt_template.format(human_prompt=prompt)\n",
    "\n",
    "  tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "  output = model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=30, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1)\n",
    "  response_1 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Inference on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zero_shot_inference:\n",
    "    rouge = Rouge()\n",
    "\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougel_scores = []\n",
    "\n",
    "    model_responses = []\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"./inferences\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Output file path\n",
    "    csv_output_file = os.path.join(output_dir, \"base_model_responses.csv\")\n",
    "\n",
    "    # Create and write to the CSV file\n",
    "    with open(csv_output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow([\"prompt\", \"expected_response\", \"model\", \"response\"])\n",
    "\n",
    "        for example in test_dataset:\n",
    "            # Extract prompt and expected response\n",
    "            prompt = example[\"text\"].split(\"### USER:\\n\")[1].split(\"\\n\\n### RESPONSE:\\n\")[0]\n",
    "            expected_response = example[\"text\"].split(\"### RESPONSE:\\n\")[1]\n",
    "\n",
    "            # Generate model's response\n",
    "            full_prompt = \"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\"\n",
    "            tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            output = model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=30, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1)\n",
    "            model_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Remove the prompt from responses\n",
    "            expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "            model_response = model_response.replace(full_prompt, \"\").strip()\n",
    "\n",
    "            if model_response == \"\":\n",
    "                model_response = \"-\"\n",
    "            \n",
    "            # Calculate ROUGE scores for model\n",
    "            scores = rouge.get_scores(model_response, expected_response)[0]\n",
    "            rouge1_scores.append(scores[\"rouge-1\"][\"f\"])\n",
    "            rouge2_scores.append(scores[\"rouge-2\"][\"f\"])\n",
    "            rougel_scores.append(scores[\"rouge-l\"][\"f\"])\n",
    "\n",
    "            # Append to CSV file\n",
    "            writer.writerow([prompt, expected_response, \"base\", model_response])\n",
    "\n",
    "    # Iterate through the test dataset again\n",
    "    for idx in range(examples_to_compare):\n",
    "        example = test_dataset[idx]\n",
    "        # Extract prompt and expected response\n",
    "        text = example[\"text\"]\n",
    "        prompt_start = text.find(\"### USER:\") + len(\"### USER:\")\n",
    "        prompt_end = text.find(\"### RESPONSE:\")\n",
    "        prompt = text[prompt_start:prompt_end].strip()\n",
    "        expected_response_start = text.find(\"### RESPONSE:\") + len(\"### RESPONSE:\")\n",
    "        expected_response = text[expected_response_start:].strip()\n",
    "        expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "        # Generate model's response and save it\n",
    "        tokens = tokenizer(\"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\", return_tensors=\"pt\")\n",
    "        model_output = model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=30, eos_token_id=tokenizer.eos_token_id)\n",
    "        model_response = tokenizer.decode(model_output[0], skip_special_tokens=True)\n",
    "        model_responses.append(model_response)\n",
    "\n",
    "    # Output file path\n",
    "    output_file = \"./eval_score/base_rouge.json\"\n",
    "    \n",
    "    # Ensure the output directory exists\n",
    "    output_file_dir = \"./eval_score\"\n",
    "    os.makedirs(output_file_dir, exist_ok=True)\n",
    "\n",
    "    # Construct data dictionary\n",
    "    data = {\n",
    "        \"Base Model Responses\": model_responses,\n",
    "        \"Base Model ROUGE Scores\": {\n",
    "            \"ROUGE-1 Scores\": rouge1_scores,\n",
    "            \"ROUGE-2 Scores\": rouge2_scores,\n",
    "            \"ROUGE-L Scores\": rougel_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write data to JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning SEA-LION 7B Instruct for Sentiment Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine_tune:\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules= [\"down_proj\", \"out_proj\", \"up_proj\", \"Wqkv\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    ft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Define tokenize function\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=81) # added max length 81 but hasn't been tested\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_datasets = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "                                    output_dir=\"train-fine-tune\",\n",
    "                                    num_train_epochs=5,\n",
    "                                    learning_rate=5e-5,\n",
    "                                    fp16=True,\n",
    "                                    overwrite_output_dir = 'True',\n",
    "                                    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=ft_model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_train_datasets,\n",
    "        eval_dataset=tokenized_val_datasets,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer_stats = trainer.train()\n",
    "\n",
    "    if saveModel:\n",
    "        ft_model.save_pretrained(\"./ft-model\")\n",
    "        trainer.push_to_hub(\"Adzka/fine-tune-sealion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuned Model Inference on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fine_tune and (ft_inference or rl_data_inference):\n",
    "  ft_model = AutoModelForCausalLM.from_pretrained(\"./ft-model\", device_map=\"auto\", trust_remote_code=True, load_in_4bit=load_in_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ft_inference and zero_shot_inference:\n",
    "  prompt_template = \"### USER:\\n{human_prompt}\\n\\n### RESPONSE:\\n\"\n",
    "\n",
    "  prompt = \"\"\"Ubah sentimen dari kalimat awal berikut menjadi sentimen sebaliknya\n",
    "  Kalimat Awal: Hari ini langitnya sangat cerah dan ramah untuk jalan-jalan\n",
    "  Kalimat Baru:\"\"\"\n",
    "\n",
    "  full_prompt = prompt_template.format(human_prompt=prompt)\n",
    "\n",
    "  tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "  output = ft_model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=20, eos_token_id=tokenizer.eos_token_id)\n",
    "  response_2 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "  print(\"Output:\")\n",
    "  print(\"--- Zero-Shot ---\")\n",
    "  print(response_1)\n",
    "  print(\"\\n--- Fine-Tuned ---\")\n",
    "  print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating responses from Test Dataset for ROUGE Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ft_inference:\n",
    "    rouge = Rouge()\n",
    "\n",
    "    ft_rouge1_scores = []\n",
    "    ft_rouge2_scores = []\n",
    "    ft_rougel_scores = []\n",
    "\n",
    "    ft_model_responses = []\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"./inferences\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Output file path\n",
    "    csv_output_file = os.path.join(output_dir, \"ft_model_responses.csv\")\n",
    "\n",
    "    # Create and write to the CSV file\n",
    "    with open(csv_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow([\"prompt\", \"expected_response\", \"model\", \"response\"])\n",
    "\n",
    "        # Iterate through the test dataset\n",
    "        i = 0\n",
    "        for example in test_dataset:\n",
    "            i += 1\n",
    "            # Extract prompt and expected response\n",
    "            prompt = example[\"text\"].split(\"### USER:\\n\")[1].split(\"\\n\\n### RESPONSE:\\n\")[0]\n",
    "            expected_response = example[\"text\"].split(\"### RESPONSE:\\n\")[1]\n",
    "\n",
    "            # Generate model's response\n",
    "            full_prompt = \"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\"\n",
    "            tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate ft_model's response\n",
    "            ft_output = ft_model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=30, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1)\n",
    "            ft_model_response = tokenizer.decode(ft_output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Remove the prompt from responses\n",
    "            expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "            ft_model_response = ft_model_response.replace(full_prompt, \"\").strip()\n",
    "\n",
    "            # Handle extra and unused responses in the model's response\n",
    "            while \"###\" in ft_model_response:\n",
    "                ft_model_response = ft_model_response.split(\"###\")[0].strip()\n",
    "            while \"\\n\\nK\" in ft_model_response:\n",
    "                ft_model_response = ft_model_response.split(\"\\n\\nK\")[0].strip()\n",
    "            while \"Kalimat Awal:\" in ft_model_response:\n",
    "                ft_model_response = ft_model_response.split(\"Kalimat Awal:\")[0].strip()\n",
    "            while \"Kalimat Baru:\" in ft_model_response:\n",
    "                ft_model_response = ft_model_response.split(\"Kalimat Baru:\")[0].strip()\n",
    "            while \" .\" in ft_model_response:\n",
    "                ft_model_response = ft_model_response.split(\" .\")[0].strip()\n",
    "                \n",
    "            if ft_model_response == \"\":\n",
    "                ft_model_response = \"-\"\n",
    "\n",
    "            print(\"Expec:\", expected_response)\n",
    "            print(\"Model:\", ft_model_response)\n",
    "\n",
    "            # Calculate ROUGE scores for ft_model\n",
    "            ft_scores = rouge.get_scores(ft_model_response, expected_response)[0]\n",
    "            ft_rouge1_scores.append(ft_scores[\"rouge-1\"][\"f\"])\n",
    "            ft_rouge2_scores.append(ft_scores[\"rouge-2\"][\"f\"])\n",
    "            ft_rougel_scores.append(ft_scores[\"rouge-l\"][\"f\"])\n",
    "\n",
    "\n",
    "            writer.writerow([prompt, expected_response, \"fine-tuned\", ft_model_response])\n",
    "            if i <= 10:\n",
    "                ft_model_responses.append(ft_model_response)\n",
    "\n",
    "    # Output file path\n",
    "    output_file = \"./eval_score/ft_rouge.json\"\n",
    "\n",
    "    # Construct data dictionary\n",
    "    data = {\n",
    "        \"Fine-Tuned Model Responses\": ft_model_responses,\n",
    "        \"Fine-Tuned Model ROUGE Scores\": {\n",
    "            \"ROUGE-1 Scores\": ft_rouge1_scores,\n",
    "            \"ROUGE-2 Scores\": ft_rouge2_scores,\n",
    "            \"ROUGE-L Scores\": ft_rougel_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Write data to JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_evaluation:\n",
    "    # Output file paths\n",
    "    base_output_file = \"./eval_score/base_rouge.json\"\n",
    "    ft_output_file = \"./eval_score/ft_rouge.json\"\n",
    "\n",
    "    # Read base model ROUGE scores\n",
    "    with open(base_output_file, \"r\") as f_base:\n",
    "        base_data = json.load(f_base)\n",
    "        rouge1_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-1 Scores\"]\n",
    "        rouge2_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-2 Scores\"]\n",
    "        rougel_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-L Scores\"]\n",
    "\n",
    "    # Read ft_model ROUGE scores\n",
    "    with open(ft_output_file, \"r\") as f_ft:\n",
    "        ft_data = json.load(f_ft)\n",
    "        rouge1_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-1 Scores\"]\n",
    "        rouge2_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-2 Scores\"]\n",
    "        rougel_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-L Scores\"]\n",
    "\n",
    "    # Calculate average ROUGE scores for model\n",
    "    avg_rouge1_base = sum(rouge1_base_scores) / len(rouge1_base_scores)\n",
    "    avg_rouge2_base = sum(rouge2_base_scores) / len(rouge2_base_scores)\n",
    "    avg_rougel_base = sum(rougel_base_scores) / len(rougel_base_scores)\n",
    "\n",
    "    # Calculate average ROUGE scores for ft_model\n",
    "    avg_rouge1_ft = sum(rouge1_ft_scores) / len(rouge1_ft_scores)\n",
    "    avg_rouge2_ft = sum(rouge2_ft_scores) / len(rouge2_ft_scores)\n",
    "    avg_rougel_ft = sum(rougel_ft_scores) / len(rougel_ft_scores)\n",
    "\n",
    "    # Print average ROUGE scores for model\n",
    "    print(\"Average ROUGE-1 F1 Score (model):\", round(avg_rouge1_base, 4))\n",
    "    print(\"Average ROUGE-2 F1 Score (model):\", round(avg_rouge2_base, 4))\n",
    "    print(\"Average ROUGE-L F1 Score (model):\", round(avg_rougel_base, 4))\n",
    "\n",
    "    # Print average ROUGE scores for ft_model\n",
    "    print(\"Average ROUGE-1 F1 Score (ft_model):\", round(avg_rouge1_ft, 4))\n",
    "    print(\"Average ROUGE-2 F1 Score (ft_model):\", round(avg_rouge2_ft, 4))\n",
    "    print(\"Average ROUGE-L F1 Score (ft_model):\", round(avg_rougel_ft, 4))\n",
    "\n",
    "    # Print evaluation results from file\n",
    "    for idx in range(examples_to_compare):\n",
    "        example = test_dataset[idx]\n",
    "        # Extract prompt and expected response\n",
    "        text = example[\"text\"]\n",
    "        prompt_start = text.find(\"### USER:\") + len(\"### USER:\")\n",
    "        prompt_end = text.find(\"### RESPONSE:\")\n",
    "        prompt = text[prompt_start:prompt_end].strip()\n",
    "        expected_response_start = text.find(\"### RESPONSE:\") + len(\"### RESPONSE:\")\n",
    "        expected_response = text[expected_response_start:].strip()\n",
    "        expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "\n",
    "        # Calculate Rouge scores for model and fine-tuned model responses\n",
    "        base_model_response = base_data[\"Base Model Responses\"][idx]\n",
    "        ft_model_response = ft_data[\"Fine-Tuned Model Responses\"][idx]\n",
    "\n",
    "        base_model_response = base_model_response.replace(\"### USER:\\n\"+prompt+\"\\n\\n### RESPONSE:\\n\",\"\").strip()\n",
    "        ft_model_response = ft_model_response.replace(\"### USER:\\n\"+prompt+\"\\n\\n### RESPONSE:\\n\",\"\").strip()\n",
    "\n",
    "        # Print the prompt and expected response\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(\"Prompt:\", prompt)\n",
    "        print(\"\\nExpected   :\", expected_response)\n",
    "\n",
    "        # Print model's response and Rouge scores\n",
    "        print(\"Base Model :\", base_model_response)\n",
    "\n",
    "        # Print fine-tuned model's response and Rouge scores\n",
    "        print(\"Fine-Tuned :\", ft_model_response)\n",
    "\n",
    "        print(\"\\n(Base Model):\")\n",
    "        print(\"ROUGE-1:\", round(rouge1_base_scores[idx], 3))\n",
    "        print(\"ROUGE-2:\", round(rouge2_base_scores[idx], 3))\n",
    "        print(\"ROUGE-L:\", round(rougel_base_scores[idx], 3))\n",
    "\n",
    "        print(\"(Fine-Tuned Model):\")\n",
    "        print(\"ROUGE-1:\", round(rouge1_ft_scores[idx], 3))\n",
    "        print(\"ROUGE-2:\", round(rouge2_ft_scores[idx], 3))\n",
    "        print(\"ROUGE-L:\", round(rougel_ft_scores[idx], 3))\n",
    "\n",
    "        print(\"\\n-----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating 4 Responses per Prompt for Reward Training from Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_4_way:\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"./inferences\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Output file path\n",
    "    csv_output_file = os.path.join(output_dir, \"ft_model_4-way_comparison.csv\")\n",
    "\n",
    "    # Create and write to the CSV file\n",
    "    n_test = len(val_dataset)\n",
    "    with open(csv_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow([\"x\", \"y0\", \"y1\", \"y2\", \"y3\"])\n",
    "\n",
    "        # Iterate through the val dataset\n",
    "        i = 0\n",
    "        for example in val_dataset:\n",
    "            i += 1\n",
    "            # Extract prompt\n",
    "            prompt = example[\"text\"].split(\"### USER:\\n\")[1].split(\"\\n\\n### RESPONSE:\\n\")[0]\n",
    "\n",
    "            # Generate model's response\n",
    "            full_prompt = \"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\"\n",
    "            tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Move input_ids to the same device as the model\n",
    "            input_ids = tokens[\"input_ids\"].to(ft_model.device)\n",
    "\n",
    "            # Generate model's response\n",
    "            ft_outputs = ft_model.generate(\n",
    "                input_ids=input_ids, \n",
    "                max_new_tokens=50, \n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                pad_token_id=tokenizer.pad_token_id, \n",
    "                num_return_sequences=4,  # Generate 4 responses\n",
    "                temperature=0.7,\n",
    "                do_sample=True  # Ensure sampling to get diverse responses\n",
    "            )\n",
    "\n",
    "            # Decode the responses\n",
    "            ft_responses = [tokenizer.decode(output, skip_special_tokens=True).replace(full_prompt, \"\").strip() for output in ft_outputs]\n",
    "\n",
    "            # Clean responses\n",
    "            clean_responses = []\n",
    "            for response in ft_responses:\n",
    "                for unwanted in [\"###\", \"\\n\\nK\", \"Kalimat Awal:\", \"Kalimat Baru:\", \" .\"]:\n",
    "                    response = response.split(unwanted)[0].strip()\n",
    "                clean_responses.append(response if response else \"-\")\n",
    "\n",
    "            # Write the prompt and the four responses to the CSV file\n",
    "            writer.writerow([prompt] + clean_responses)\n",
    "\n",
    "            # Output progress\n",
    "            print(f\"Processed example {i} out of {n_test}\")\n",
    "\n",
    "    print(\"Responses successfully written to ft_model_4-way_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating 4 Responses per Prompt for PPO from Reused Validation Dataset -- Akan dinilai oleh Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rl_data_inference:\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"./inferences\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Output file path\n",
    "    csv_output_file = os.path.join(output_dir, \"ft_model_rl_data_for_PPO.csv\")\n",
    "\n",
    "    # Create and write to the CSV file\n",
    "    n_test = len(rl_dataset)\n",
    "    n_sample = 3  # Number of responses per prompt\n",
    "\n",
    "    with open(csv_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow([\"query\", \"response\"])\n",
    "\n",
    "        # Iterate through the val dataset\n",
    "        i = 0\n",
    "        for example in rl_dataset:\n",
    "            i += 1\n",
    "            # Extract prompt\n",
    "            prompt = example[\"text\"].split(\"### USER:\\n\")[1].split(\"\\n\\n### RESPONSE:\\n\")[0]\n",
    "\n",
    "            # Generate model's response\n",
    "            full_prompt = \"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\"\n",
    "            tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Move input_ids to the same device as the model\n",
    "            input_ids = tokens[\"input_ids\"].to(ft_model.device)\n",
    "\n",
    "            # Generate model's response\n",
    "            ft_outputs = ft_model.generate(\n",
    "                input_ids=input_ids, \n",
    "                max_new_tokens=50, \n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                pad_token_id=tokenizer.pad_token_id, \n",
    "                num_return_sequences=n_sample,  # Generate specified number of responses\n",
    "                temperature=0.7,\n",
    "                do_sample=True  # Ensure sampling to get diverse responses\n",
    "            )\n",
    "\n",
    "            # Decode the responses\n",
    "            ft_responses = [tokenizer.decode(output, skip_special_tokens=True).replace(full_prompt, \"\").strip() for output in ft_outputs]\n",
    "\n",
    "            # Clean responses and ensure uniqueness\n",
    "            clean_responses = []\n",
    "            for response in ft_responses:\n",
    "                for unwanted in [\"###\", \"\\n\\nK\", \"Kalimat Awal:\", \"Kalimat Baru:\", \" .\"]:\n",
    "                    response = response.split(unwanted)[0].strip()\n",
    "                if response and response not in clean_responses:\n",
    "                    clean_responses.append(response)\n",
    "\n",
    "            # Write the prompt and the unique responses to the CSV file\n",
    "            for response in clean_responses:\n",
    "                writer.writerow([prompt, response])\n",
    "\n",
    "            # Output progress\n",
    "            print(f\"Processed example {i} out of {n_test}\")\n",
    "            if i % 30 == 0 and i != 0:\n",
    "                time.sleep(20)\n",
    "\n",
    "    print(\"Responses successfully written to ft_model_rl_data_for_PPO.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
