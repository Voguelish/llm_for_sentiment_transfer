{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoConfig, AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_config_from_source = False\n",
    "generate_test_response = False\n",
    "ppo_training = True\n",
    "rlhf_inference = True\n",
    "print_evaluation = False\n",
    "\n",
    "examples_to_compare = 5\n",
    "\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_config_from_source:\n",
    "  # Load the original model configuration\n",
    "  original_model_name = \"aisingapore/sea-lion-7b-instruct\"\n",
    "  original_config = AutoConfig.from_pretrained(original_model_name)\n",
    "\n",
    "  # Save the configuration to the fine-tuned model directory\n",
    "  original_config.save_pretrained(\"./ft-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration of the fine-tuned model\n",
    "config = AutoConfig.from_pretrained(\"./ft-model\")\n",
    "\n",
    "class CustomValueHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        hidden_size = getattr(config, 'd_model', None)  # Use 'd_model' from the configuration\n",
    "        if hidden_size is None:\n",
    "            raise ValueError(\"The configuration does not have 'd_model' attribute.\")\n",
    "        self.summary = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states: Tensor) -> Tensor:\n",
    "        # Ensure input and weights have the same dtype\n",
    "        hidden_states = hidden_states.to(self.summary.weight.dtype)\n",
    "        # Flatten the tensor and ensure it matches the expected input shape for the linear layer\n",
    "        batch_size, seq_len, hidden_size = hidden_states.size()\n",
    "        x = hidden_states.view(batch_size * seq_len, hidden_size)\n",
    "        x = self.summary(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        return x\n",
    "\n",
    "class CustomAutoModelForCausalLMWithValueHead(AutoModelForCausalLMWithValueHead):\n",
    "    def __init__(self, pretrained_model, **kwargs):\n",
    "        super(AutoModelForCausalLMWithValueHead, self).__init__(pretrained_model, **kwargs)\n",
    "        self.v_head = CustomValueHead(pretrained_model.config)\n",
    "        self.is_peft_model = False \n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
    "        # Load the pretrained model\n",
    "        pretrained_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "        # Return the custom class\n",
    "        return cls(pretrained_model, **kwargs)\n",
    "\n",
    "model = CustomAutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    \"./ft-model\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tokenizer = AutoTokenizer.from_pretrained(\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
    "reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"./reward_model\", num_labels=3, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"aisingapore/sea-lion-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_path = \"./inferences/ft_model_rl_data_for_PPO_scored.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Extract the 'score' column\n",
    "scores = df['score']\n",
    "\n",
    "# Calculate min and max scores\n",
    "min_score = scores.min()\n",
    "max_score = scores.max()\n",
    "\n",
    "print(f\"Min Score: {min_score}\")\n",
    "print(f\"Max Score: {max_score}\")\n",
    "\n",
    "# Function to get reward score\n",
    "def get_reward_score(response):\n",
    "    tokens = reward_tokenizer(response, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"].to(reward_model.device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(reward_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    summed_logits = logits.sum(dim=-1).cpu().tolist()\n",
    "    \n",
    "    # Normalize the score between 0 and 1\n",
    "    normalized_score = (summed_logits[0] - min_score) / (max_score - min_score)\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "dataset_path = \"./dataset/rl_data.json\"\n",
    "df = pd.read_json(dataset_path)\n",
    "\n",
    "def extract_query(text):\n",
    "    start_idx = text.find(\"### USER:\")\n",
    "    end_idx = text.find(\"### RESPONSE:\")\n",
    "    return text[start_idx:end_idx+13]\n",
    "\n",
    "df['query'] = df['text'].apply(extract_query)\n",
    "new_df = df[['query']]\n",
    "dataset = Dataset.from_pandas(new_df)\n",
    "\n",
    "# Tokenization with padding and truncation\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"query\"], padding=\"max_length\", truncation=True, max_length=max_length, return_attention_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Ensure padding token is set correctly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"./ft-model\",\n",
    "    learning_rate=1.41e-5,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 30,\n",
    "}\n",
    "\n",
    "# Initialize lists to store the tensors\n",
    "query_tensors = []\n",
    "response_tensors = []\n",
    "rewards = []\n",
    "\n",
    "# Function to get reward score\n",
    "def get_reward_score(text):\n",
    "    tokens = reward_tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = tokens[\"input_ids\"].to(reward_model.device)\n",
    "    attention_mask = tokens[\"attention_mask\"].to(reward_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    rewards = logits.softmax(dim=-1)[:, 1].cpu().tolist()\n",
    "    return rewards[0]\n",
    "\n",
    "print(len(tokenized_dataset))\n",
    "if ppo_training:\n",
    "    x = 1\n",
    "    for i in range(len(tokenized_dataset)):\n",
    "        query = tokenized_dataset[i][\"input_ids\"]\n",
    "        attention_mask = tokenized_dataset[i][\"attention_mask\"]\n",
    "\n",
    "        input_ids = torch.tensor(query).unsqueeze(0).to(next(model.parameters()).device)\n",
    "        attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(next(model.parameters()).device)\n",
    "        \n",
    "        # Generate response\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        original_query = tokenizer.decode(query, skip_special_tokens=True)\n",
    "        response = response.replace(original_query, \"\").strip()\n",
    "\n",
    "        # Get reward score\n",
    "        reward = get_reward_score(original_query + response)\n",
    "\n",
    "        # Convert query and response to tensors\n",
    "        query_tensor = torch.tensor(query).to(next(model.parameters()).device)\n",
    "        response_tensor = torch.tensor(tokenizer.encode(response)).to(next(model.parameters()).device)\n",
    "        \n",
    "        # Ensure padding of response_tensor\n",
    "        response_tensor = torch.nn.functional.pad(response_tensor, (0, max_length - response_tensor.size(0)), value=tokenizer.pad_token_id)\n",
    "\n",
    "        # Store tensors and reward in lists\n",
    "        query_tensors.append(query_tensor)\n",
    "        response_tensors.append(response_tensor)\n",
    "        rewards.append(torch.tensor([reward]).to(next(model.parameters()).device))\n",
    "        if i % 30 == 0:\n",
    "            print(i, \"out of\", len(tokenized_dataset))\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = config.batch_size\n",
    "\n",
    "    # Split data into batches and ensure they are lists of tensors\n",
    "    for i in range(0, len(query_tensors), batch_size):\n",
    "        query_batch = query_tensors[i:i + batch_size]\n",
    "        response_batch = response_tensors[i:i + batch_size]\n",
    "        reward_batch = rewards[i:i + batch_size]\n",
    "\n",
    "        if len(query_batch) < batch_size:\n",
    "            print(\"Skipping last batch due to less than batch size.\")\n",
    "            continue\n",
    "\n",
    "        # Use these tensors with ppo_trainer\n",
    "        stats = ppo_trainer.step(query_batch, response_batch, reward_batch)\n",
    "\n",
    "        # Log stats\n",
    "        ppo_trainer.log_stats(stats, {}, reward_batch)\n",
    "        print(\"Batch processed:\", i+8, \"of\", len(query_tensors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "if ppo_training:\n",
    "  ppo_trainer.save_pretrained(\"rlhf-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from rouge import Rouge\n",
    "\n",
    "# Check if rlhf_inference is True and ppo_training is False\n",
    "if rlhf_inference and not ppo_training:\n",
    "    model_name = \"./rlhf-model\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, load_in_4bit=True)\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Load the evaluation dataset\n",
    "    test_dataset = load_dataset(\"json\", data_files=\"./dataset/rl_eval.json\")[\"train\"]\n",
    "\n",
    "    # Set up ROUGE scoring\n",
    "    rouge = Rouge()\n",
    "    rlhf_rouge1_scores = []\n",
    "    rlhf_rouge2_scores = []\n",
    "    rlhf_rougel_scores = []\n",
    "    rlhf_model_responses = []\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = \"./inferences\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_output_file = os.path.join(output_dir, \"rlhf_model_responses.csv\")\n",
    "\n",
    "    # Create and write to the CSV file\n",
    "    with open(csv_output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"expected_response\", \"model\", \"response\"])\n",
    "\n",
    "        # Iterate through the evaluation dataset\n",
    "        for example in test_dataset:\n",
    "            # Extract prompt and expected response\n",
    "            prompt = example[\"text\"].split(\"### USER:\\n\")[1].split(\"\\n\\n### RESPONSE:\\n\")[0]\n",
    "            expected_response = example[\"text\"].split(\"### RESPONSE:\\n\")[1]\n",
    "\n",
    "            # Generate model's response\n",
    "            full_prompt = \"### USER:\\n\" + prompt + \"\\n\\n### RESPONSE:\\n\"\n",
    "            tokens = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate rlhf_model's response\n",
    "            rlhf_output = model.generate(input_ids=tokens[\"input_ids\"], max_new_tokens=30, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1)\n",
    "            rlhf_model_response = tokenizer.decode(rlhf_output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Remove the prompt from responses\n",
    "            expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "            rlhf_model_response = rlhf_model_response.replace(full_prompt, \"\").strip()\n",
    "\n",
    "            # Handle extra and unused responses in the model's response\n",
    "            while \"###\" in rlhf_model_response:\n",
    "                rlhf_model_response = rlhf_model_response.split(\"###\")[0].strip()\n",
    "            while \"\\n\\nK\" in rlhf_model_response:\n",
    "                rlhf_model_response = rlhf_model_response.split(\"\\n\\nK\")[0].strip()\n",
    "            while \"Kalimat Awal:\" in rlhf_model_response:\n",
    "                rlhf_model_response = rlhf_model_response.split(\"Kalimat Awal:\")[0].strip()\n",
    "            while \"Kalimat Baru:\" in rlhf_model_response:\n",
    "                rlhf_model_response = rlhf_model_response.split(\"Kalimat Baru:\")[0].strip()\n",
    "            while \" .\" in rlhf_model_response:\n",
    "                rlhf_model_response = rlhf_model_response.split(\" .\")[0].strip()\n",
    "\n",
    "            if rlhf_model_response == \"\":\n",
    "                rlhf_model_response = \"-\"\n",
    "\n",
    "            print(\"Expec:\", expected_response)\n",
    "            print(\"Model:\", rlhf_model_response)\n",
    "\n",
    "            # Calculate ROUGE scores for rlhf_model\n",
    "            rlhf_scores = rouge.get_scores(rlhf_model_response, expected_response)[0]\n",
    "            rlhf_rouge1_scores.append(rlhf_scores[\"rouge-1\"][\"f\"])\n",
    "            rlhf_rouge2_scores.append(rlhf_scores[\"rouge-2\"][\"f\"])\n",
    "            rlhf_rougel_scores.append(rlhf_scores[\"rouge-l\"][\"f\"])\n",
    "\n",
    "            writer.writerow([prompt, expected_response, \"rlhf\", rlhf_model_response])\n",
    "            rlhf_model_responses.append(rlhf_model_response)\n",
    "\n",
    "    # Save ROUGE scores to a JSON file\n",
    "    output_file = \"./eval_score/rlhf_rouge.json\"\n",
    "    data = {\n",
    "        \"RLHF Model Responses\": rlhf_model_responses,\n",
    "        \"RLHF Model ROUGE Scores\": {\n",
    "            \"ROUGE-1 Scores\": rlhf_rouge1_scores,\n",
    "            \"ROUGE-2 Scores\": rlhf_rouge2_scores,\n",
    "            \"ROUGE-L Scores\": rlhf_rougel_scores\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_evaluation:\n",
    "    test_dataset = load_dataset(\"json\", data_files=\"./dataset/rl_eval.json\")[\"train\"]\n",
    "    # Output file paths\n",
    "    base_output_file = \"./eval_score/base_rouge.json\"\n",
    "    ft_output_file = \"./eval_score/ft_rouge.json\"\n",
    "    rlhf_output_file = \"./eval_score/rlhf_rouge.json\"\n",
    "\n",
    "    # Read base model ROUGE scores\n",
    "    with open(base_output_file, \"r\") as f_base:\n",
    "        base_data = json.load(f_base)\n",
    "        rouge1_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-1 Scores\"]\n",
    "        rouge2_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-2 Scores\"]\n",
    "        rougel_base_scores = base_data[\"Base Model ROUGE Scores\"][\"ROUGE-L Scores\"]\n",
    "\n",
    "    # Read ft_model ROUGE scores\n",
    "    with open(ft_output_file, \"r\") as f_ft:\n",
    "        ft_data = json.load(f_ft)\n",
    "        rouge1_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-1 Scores\"]\n",
    "        rouge2_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-2 Scores\"]\n",
    "        rougel_ft_scores = ft_data[\"Fine-Tuned Model ROUGE Scores\"][\"ROUGE-L Scores\"]\n",
    "\n",
    "    # Read rlhf_model ROUGE scores\n",
    "    with open(rlhf_output_file, \"r\") as f_rlhf:\n",
    "        rlhf_data = json.load(f_rlhf)\n",
    "        rouge1_rlhf_scores = rlhf_data[\"RLHF Model ROUGE Scores\"][\"ROUGE-1 Scores\"]\n",
    "        rouge2_rlhf_scores = rlhf_data[\"RLHF Model ROUGE Scores\"][\"ROUGE-2 Scores\"]\n",
    "        rougel_rlhf_scores = rlhf_data[\"RLHF Model ROUGE Scores\"][\"ROUGE-L Scores\"]\n",
    "\n",
    "    # Calculate average ROUGE scores for models\n",
    "    avg_rouge1_base = sum(rouge1_base_scores) / len(rouge1_base_scores)\n",
    "    avg_rouge2_base = sum(rouge2_base_scores) / len(rouge2_base_scores)\n",
    "    avg_rougel_base = sum(rougel_base_scores) / len(rougel_base_scores)\n",
    "\n",
    "    avg_rouge1_ft = sum(rouge1_ft_scores) / len(rouge1_ft_scores)\n",
    "    avg_rouge2_ft = sum(rouge2_ft_scores) / len(rouge2_ft_scores)\n",
    "    avg_rougel_ft = sum(rougel_ft_scores) / len(rougel_ft_scores)\n",
    "\n",
    "    avg_rouge1_rlhf = sum(rouge1_rlhf_scores) / len(rouge1_rlhf_scores)\n",
    "    avg_rouge2_rlhf = sum(rouge2_rlhf_scores) / len(rouge2_rlhf_scores)\n",
    "    avg_rougel_rlhf = sum(rougel_rlhf_scores) / len(rougel_rlhf_scores)\n",
    "\n",
    "    # Print average ROUGE scores for models\n",
    "    print(\"Average ROUGE-1 F1 Score (model):\", round(avg_rouge1_base, 4))\n",
    "    print(\"Average ROUGE-2 F1 Score (model):\", round(avg_rouge2_base, 4))\n",
    "    print(\"Average ROUGE-L F1 Score (model):\", round(avg_rougel_base, 4))\n",
    "\n",
    "    print(\"Average ROUGE-1 F1 Score (ft_model):\", round(avg_rouge1_ft, 4))\n",
    "    print(\"Average ROUGE-2 F1 Score (ft_model):\", round(avg_rouge2_ft, 4))\n",
    "    print(\"Average ROUGE-L F1 Score (ft_model):\", round(avg_rougel_ft, 4))\n",
    "\n",
    "    print(\"Average ROUGE-1 F1 Score (rlhf_model):\", round(avg_rouge1_rlhf, 4))\n",
    "    print(\"Average ROUGE-2 F1 Score (rlhf_model):\", round(avg_rouge2_rlhf, 4))\n",
    "    print(\"Average ROUGE-L F1 Score (rlhf_model):\", round(avg_rougel_rlhf, 4))\n",
    "\n",
    "    # Print evaluation results from file\n",
    "    for idx in range(examples_to_compare):\n",
    "        example = test_dataset[idx]\n",
    "        # Extract prompt and expected response\n",
    "        text = example[\"text\"]\n",
    "        prompt_start = text.find(\"### USER:\") + len(\"### USER:\")\n",
    "        prompt_end = text.find(\"### RESPONSE:\")\n",
    "        prompt = text[prompt_start:prompt_end].strip()\n",
    "        expected_response_start = text.find(\"### RESPONSE:\") + len(\"### RESPONSE:\")\n",
    "        expected_response = text[expected_response_start:].strip()\n",
    "        expected_response = expected_response.replace(\"<|endoftext|>\", \"\").strip()\n",
    "\n",
    "        # Calculate ROUGE scores for base model, fine-tuned model, and RLHF model responses\n",
    "        base_model_response = base_data[\"Base Model Responses\"][idx]\n",
    "        ft_model_response = ft_data[\"Fine-Tuned Model Responses\"][idx]\n",
    "        rlhf_model_response = rlhf_data[\"RLHF Model Responses\"][idx]\n",
    "\n",
    "        # Simplify base model response to only include the sentiment\n",
    "        base_model_response = base_model_response.split(\"\\n### RESPONSE:\\n\")[-1].strip()\n",
    "\n",
    "        # Print the prompt and expected response\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(\"Prompt:\", prompt)\n",
    "        print(\"\\nExpected   :\", expected_response)\n",
    "\n",
    "        # Print base model's response and ROUGE scores\n",
    "        print(\"Base Model :\", base_model_response)\n",
    "\n",
    "        # Print fine-tuned model's response and ROUGE scores\n",
    "        print(\"Fine-Tuned :\", ft_model_response)\n",
    "\n",
    "        # Print RLHF model's response and ROUGE scores\n",
    "        print(\"RLHF Model :\", rlhf_model_response)\n",
    "\n",
    "        print(\"\\n(Base Model):\")\n",
    "        print(\"ROUGE-1:\", round(rouge1_base_scores[idx], 3))\n",
    "        print(\"ROUGE-2:\", round(rouge2_base_scores[idx], 3))\n",
    "        print(\"ROUGE-L:\", round(rougel_base_scores[idx], 3))\n",
    "\n",
    "        print(\"(Fine-Tuned Model):\")\n",
    "        print(\"ROUGE-1:\", round(rouge1_ft_scores[idx], 3))\n",
    "        print(\"ROUGE-2:\", round(rouge2_ft_scores[idx], 3))\n",
    "        print(\"ROUGE-L:\", round(rougel_ft_scores[idx], 3))\n",
    "\n",
    "        print(\"(RLHF Model):\")\n",
    "        print(\"ROUGE-1:\", round(rouge1_rlhf_scores[idx], 3))\n",
    "        print(\"ROUGE-2:\", round(rouge2_rlhf_scores[idx], 3))\n",
    "        print(\"ROUGE-L:\", round(rougel_rlhf_scores[idx], 3))\n",
    "\n",
    "        print(\"\\n-----------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
